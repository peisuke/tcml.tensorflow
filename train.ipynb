{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('data.npy')\n",
    "data = np.reshape(data,[-1,20,28,28])\n",
    "train_data = data[:1200,:,:,:]\n",
    "test_data = data[1200:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_input(batch_y, nb_batch, nb_class):    \n",
    "    batch_p = (np.arange(nb_class) == batch_y[:,:-1,None]).astype(int)\n",
    "    dummy = np.zeros((nb_batch, 1, nb_class), dtype=np.float32)\n",
    "    return np.concatenate((dummy, batch_p), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(nb_batch, nb_episode, nb_class, data):\n",
    "    dshape = data.shape[2:]\n",
    "    batch_x = np.zeros((nb_batch, nb_episode) + dshape + (1,))\n",
    "    batch_y = np.zeros((nb_batch, nb_episode), dtype=np.int)\n",
    "    batch_mask = np.ones((nb_batch, nb_episode), dtype=np.bool)\n",
    "    nb_class_total = len(data)\n",
    "    \n",
    "    for i in range(nb_batch):\n",
    "        # classes for learning\n",
    "        classes = np.random.choice(nb_class_total, nb_class, False)\n",
    "        \n",
    "        # index to class\n",
    "        # class_idx is classes[pinds[i]]\n",
    "        pidx = np.random.permutation(nb_class)\n",
    "        \n",
    "        # sample data\n",
    "        sample = np.random.randint(0, nb_class, nb_episode)\n",
    "        batch_y[i] = sample\n",
    "        \n",
    "        _, first = np.unique(sample, return_index=True)\n",
    "        mask = np.ones(nb_episode, np.bool)\n",
    "        mask[first] = False\n",
    "        batch_mask[i] = mask\n",
    "        \n",
    "        for j in range(nb_class):\n",
    "            idx = (sample == j)\n",
    "            eidx = np.random.choice(data.shape[1], np.sum(sample == j), False)\n",
    "            imgs = data[classes[pidx[j]], eidx]\n",
    "            \n",
    "            batch_x[i, idx, :, :, 0] = np.rot90(imgs, np.random.randint(4), axes=(1,2))\n",
    "            \n",
    "    # generate previous label\n",
    "    batch_p = make_input(batch_y, nb_batch, nb_class)\n",
    "    \n",
    "    return batch_x, batch_p, batch_y, batch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_episode = 32\n",
    "nb_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embd_net(inp, scope, reuse=False, stop_grad=False):\n",
    "    nb_episode = int(inp.shape[1])\n",
    "    \n",
    "    with tf.variable_scope(scope) as varscope:\n",
    "        if reuse: \n",
    "            varscope.reuse_variables()\n",
    "\n",
    "        _inp = tf.reshape(inp, [-1, 28, 28, 1])\n",
    "        cur_input = _inp\n",
    "        cur_filters = 1\n",
    "        \n",
    "        for i in range(4):\n",
    "            with tf.variable_scope('conv'+str(i)):\n",
    "                W = tf.get_variable('W', [3, 3, cur_filters, 64])\n",
    "                beta = tf.get_variable('beta', [64], initializer=tf.constant_initializer(0.0))\n",
    "                gamma = tf.get_variable('gamma', [64], initializer=tf.constant_initializer(1.0))\n",
    "\n",
    "                cur_filters = 64\n",
    "                pre_norm = tf.nn.conv2d(cur_input, W, strides=[1,1,1,1], padding='SAME')\n",
    "                mean, variance = tf.nn.moments(pre_norm, [0, 1, 2])\n",
    "                post_norm = tf.nn.batch_normalization(pre_norm, mean, variance, beta, gamma, variance_epsilon = 1e-10)\n",
    "                conv = tf.nn.relu(post_norm)\n",
    "                cur_input = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding = 'VALID')\n",
    "\n",
    "        if stop_grad:\n",
    "            squeezed = tf.squeeze(cur_input, [1,2])\n",
    "            output = tf.stop_gradient(tf.reshape(squeezed, [-1, nb_episode, 64]))\n",
    "        else:\n",
    "            squeezed = tf.squeeze(cur_input, [1,2])\n",
    "            output = tf.reshape(squeezed, [-1, nb_episode, 64])\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calual_conv_with_activation(inp, nb_input, nb_output, dilation_rate):\n",
    "        Wf = tf.get_variable('W_filter', [2, nb_input, nb_output])\n",
    "        bf = tf.get_variable('b_filter', [nb_output])    \n",
    "        Wg = tf.get_variable('W_gate', [2, nb_input, nb_output])\n",
    "        bg = tf.get_variable('b_gate', [nb_output])            \n",
    "        \n",
    "        x = tf.pad(inp, [[0, 0], [dilation_rate, 0], [0, 0]])\n",
    "        \n",
    "        xf = tf.nn.convolution(x, Wf, strides=[1,], dilation_rate=[dilation_rate,], padding='VALID')\n",
    "        xf = tf.nn.bias_add(xf, bf)\n",
    "        \n",
    "        xg = tf.nn.convolution(x, Wg, strides=[1,], dilation_rate=[dilation_rate,], padding='VALID')\n",
    "        xg = tf.nn.bias_add(xg, bg)\n",
    "        \n",
    "        out = tf.tanh(xf) * tf.sigmoid(xg)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_block(inp, nb_dim, dilation_rate, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = calual_conv_with_activation(inp, nb_dim, nb_dim, dilation_rate)\n",
    "        x = x + inp\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_block(inp, nb_dim, dilation_rate, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = calual_conv_with_activation(inp, nb_dim, 128, dilation_rate)\n",
    "        x = res_block(x, 128, dilation_rate, 'res_01')\n",
    "        x = res_block(x, 128, dilation_rate, 'res_02')\n",
    "        \n",
    "        x = tf.concat((inp, x), axis=2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tcml(inp, label, nb_class, scope, reuse=False, stop_grad=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        with tf.variable_scope('preprocess'):\n",
    "            x = tf.concat((inp, label), axis=2)\n",
    "\n",
    "        nb_channel = int(x.shape[2])\n",
    "        x = dense_block(x, nb_channel+0*128, 1, 'dense_01')\n",
    "        x = dense_block(x, nb_channel+1*128, 2, 'dense_02')\n",
    "        x = dense_block(x, nb_channel+2*128, 4, 'dense_03')\n",
    "        x = dense_block(x, nb_channel+3*128, 8, 'dense_04')\n",
    "        x = dense_block(x, nb_channel+4*128, 16, 'dense_05')\n",
    "        x = dense_block(x, nb_channel+5*128, 1, 'dense_06',)\n",
    "        x = dense_block(x, nb_channel+6*128, 2, 'dense_07',)\n",
    "        x = dense_block(x, nb_channel+7*128, 4, 'dense_08')\n",
    "        x = dense_block(x, nb_channel+8*128, 8, 'dense_09')\n",
    "        x = dense_block(x, nb_channel+9*128, 16, 'dense_10')\n",
    "        \n",
    "        with tf.variable_scope('postprocess'):\n",
    "            W1 = tf.get_variable('W1', [1, nb_channel+10*128, 512])\n",
    "            b1 = tf.get_variable('b1', [512])\n",
    "            W2 = tf.get_variable('W2', [1, 512, nb_class])\n",
    "            b2 = tf.get_variable('b2', [nb_class])\n",
    "\n",
    "            x = tf.nn.conv1d(x, W1, stride=1, padding='SAME')\n",
    "            x = tf.nn.bias_add(x, b1)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x = tf.nn.conv1d(x, W2, stride=1, padding='SAME')            \n",
    "            x = tf.nn.bias_add(x, b2)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build(img, prev_label, nb_class):\n",
    "    feature = embd_net(img, 'embd')\n",
    "    tcml = build_tcml(feature, prev_label, nb_class, 'TCML')\n",
    "    \n",
    "    return tcml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, shape=[None, nb_episode, 28, 28, 1])\n",
    "prev_label = tf.placeholder(tf.float32, shape=[None, nb_episode, 5])\n",
    "train_label = tf.placeholder(tf.int32, shape=[None, nb_episode])\n",
    "valid_label = tf.placeholder(tf.bool, shape=[None, nb_episode])\n",
    "\n",
    "net = build(img, prev_label, nb_class)\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    y = tf.boolean_mask(net, valid_label)\n",
    "    t = tf.one_hot(tf.boolean_mask(train_label, valid_label), 5)\n",
    "\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=y)\n",
    "    loss = tf.reduce_mean(cross_entropy, name='cross_entropy_mean')\n",
    "    tf.add_to_collection('losses', loss)\n",
    "\n",
    "    cost = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    train_step = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter('./log', graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(train_data)\n",
    "batchsize = 8\n",
    "perm = np.random.permutation(N)\n",
    "\n",
    "for i in range(0, N, batchsize):\n",
    "    batch_x, batch_p, batch_y, batch_m = get_minibatch(batchsize, nb_episode, nb_class, train_data)\n",
    "    \n",
    "    feed_dict = {\n",
    "        img: batch_x,\n",
    "        prev_label: batch_p,\n",
    "        train_label: batch_y,\n",
    "        valid_label: batch_m\n",
    "    }\n",
    "    \n",
    "    _, loss = sess.run([train_step, cost], feed_dict=feed_dict)\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
